#!/usr/bin/env python3
import argparse
import csv
import hashlib
import json
from pathlib import Path


def main() -> int:
    parser = argparse.ArgumentParser(description="Build BIG5 zh-CN xu bootstrap norms CSV and merge into seed file.")
    parser.add_argument(
        "--input",
        default="backend/content_packs/BIG5_OCEAN/v1/raw/norm_stats.csv",
        help="Input raw norm_stats.csv path",
    )
    parser.add_argument(
        "--seed",
        default="backend/resources/norms/big5/big5_norm_stats_seed.csv",
        help="Seed CSV path generated by en bootstrap script",
    )
    parser.add_argument(
        "--artifact",
        default="backend/resources/norms/big5/build_artifacts/2026Q1_xu_v1__zh-CN_xu_all_18-60.json",
        help="Output build artifact json path",
    )
    args = parser.parse_args()

    root = Path(__file__).resolve().parents[2]
    in_path = (root.parent / args.input).resolve() if args.input.startswith("backend/") else (root / args.input).resolve()
    seed_path = (root.parent / args.seed).resolve() if args.seed.startswith("backend/") else (root / args.seed).resolve()
    artifact_path = (root.parent / args.artifact).resolve() if args.artifact.startswith("backend/") else (root / args.artifact).resolve()

    zh_domains = []
    global_facets = []
    with in_path.open("r", encoding="utf-8", newline="") as f:
        reader = csv.DictReader(f)
        for row in reader:
            gid = (row.get("group_id") or "").strip()
            level = (row.get("metric_level") or "").strip().lower()
            code = (row.get("metric_code") or "").strip().upper()

            if gid == "zh-CN_all" and level == "domain":
                zh_domains.append((level, code, row))
            if gid == "global_all" and level == "facet":
                global_facets.append((level, code, row))

    if len(zh_domains) != 5:
        raise SystemExit(f"expected 5 zh-CN domain rows, got {len(zh_domains)}")
    if len(global_facets) != 30:
        raise SystemExit(f"expected 30 global facet rows, got {len(global_facets)}")

    fieldnames = [
        "scale_code", "norms_version", "locale", "region", "group_id", "gender", "age_min", "age_max",
        "metric_level", "metric_code", "mean", "sd", "sample_n", "source_id", "source_type", "status",
        "is_active", "published_at",
    ]

    zh_rows = []
    for level, code, row in zh_domains + global_facets:
        zh_rows.append({
            "scale_code": "BIG5_OCEAN",
            "norms_version": "2026Q1_xu_v1",
            "locale": "zh-CN",
            "region": "CN_MAINLAND",
            "group_id": "zh-CN_xu_all_18-60",
            "gender": "ALL",
            "age_min": "18",
            "age_max": "60",
            "metric_level": level,
            "metric_code": code,
            "mean": f"{float(row.get('mean') or 0):.3f}",
            "sd": f"{float(row.get('sd') or 0):.3f}",
            "sample_n": str(int(float(row.get("sample_n") or 0))),
            "source_id": "ZH_CN_IPIPNEO120_XU",
            "source_type": "peer_reviewed",
            "status": "BOOTSTRAP",
            "is_active": "1",
            "published_at": "2026-02-21T00:00:00Z",
        })

    merged = []
    if seed_path.exists():
        with seed_path.open("r", encoding="utf-8", newline="") as f:
            reader = csv.DictReader(f)
            for row in reader:
                if (row.get("group_id") or "") == "zh-CN_xu_all_18-60":
                    continue
                merged.append(row)

    merged.extend(zh_rows)

    merged.sort(key=lambda r: (
        r.get("group_id", ""),
        0 if r.get("metric_level") == "domain" else 1,
        r.get("metric_code", ""),
    ))

    seed_path.parent.mkdir(parents=True, exist_ok=True)
    with seed_path.open("w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(merged)

    payload = seed_path.read_bytes()
    out_hash = hashlib.sha256(payload).hexdigest()
    artifact = {
        "scale_code": "BIG5_OCEAN",
        "norms_version": "2026Q1_xu_v1",
        "source_id": "ZH_CN_IPIPNEO120_XU",
        "source_type": "peer_reviewed",
        "pack_locale": "zh-CN",
        "group_id": "zh-CN_xu_all_18-60",
        "sample_n_raw": int(zh_rows[0]["sample_n"]),
        "sample_n_kept": int(zh_rows[0]["sample_n"]),
        "filters_applied": {
            "quality_levels": ["A", "B"],
            "source_mode": "fallback_norm_rows",
            "input_path": str(in_path),
        },
        "compute_spec_hash": hashlib.sha256(b"big5_spec_2026Q1_v1").hexdigest(),
        "output_csv_sha256": out_hash,
        "output_csv_path": str(seed_path),
    }
    artifact_path.parent.mkdir(parents=True, exist_ok=True)
    artifact_path.write_text(json.dumps(artifact, ensure_ascii=False, indent=2), encoding="utf-8")

    print(f"[big5_build_zhcn_xu_bootstrap] merged rows={len(merged)} -> {seed_path} sha256={out_hash}")
    print(f"[big5_build_zhcn_xu_bootstrap] artifact -> {artifact_path}")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
